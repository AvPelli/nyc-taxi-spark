{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c45d274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Check JAVA_HOME\n",
    "print(\"JAVA_HOME:\", os.environ.get('JAVA_HOME', 'Not set'))\n",
    "\n",
    "if os.environ.get('JAVA_HOME'):\n",
    "    result = subprocess.run([os.environ['JAVA_HOME'] + '/bin/java', '-version'], \n",
    "                          capture_output=True, text=True)\n",
    "    print(\"\\nProject JAVA_HOME Java version:\")\n",
    "    print(result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e44b419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"local-pyspark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36007b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read csv\n",
    "taxi_df = pd.read_csv(\"./nyc-yellow-taxi-trip-records-january-2024/nyc_tlc_yellow_2024_01.csv\")\n",
    "\n",
    "print(taxi_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e49d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to parquet\n",
    "\n",
    "taxi_df.to_parquet(\"./nyc-yellow-taxi-trip-records-january-2024/nyc_tlc_yellow_2024_01.parquet\")\n",
    "taxi_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9641af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Check generated statistics in parquet file\n",
    "pf = pq.ParquetFile(\"./nyc-yellow-taxi-trip-records-january-2024/nyc_tlc_yellow_2024_01.parquet\")\n",
    "print(f\"Row groups: {pf.num_row_groups}\")\n",
    "for i in range(pf.num_row_groups):\n",
    "    rg = pf.metadata.row_group(i)\n",
    "    print(type(rg))\n",
    "\n",
    "    print(f\"\\n--- Row Group {i} ({rg.num_rows:,} rows) ---\")\n",
    "    \n",
    "    for j in range(rg.num_columns):\n",
    "        print(f\"\\n--- Column {j} statistics ---\")\n",
    "        chunk = rg.column(j)\n",
    "        print(chunk.statistics.to_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e99add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time for benchmarking\n",
    "import time\n",
    "\n",
    "def timer(func, df, *args, **kwargs):\n",
    "    \"\"\"Time Spark operation and return result\"\"\"\n",
    "    start = time.perf_counter()\n",
    "    result = func(df, *args, **kwargs)\n",
    "    elapsed = (time.perf_counter() - start) * 1000\n",
    "    print(f\"Time: {elapsed:.2f} ms\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994961ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read parquet file in Spark session\n",
    "taxi_pq_df = spark.read.parquet(\"./nyc-yellow-taxi-trip-records-january-2024/nyc_tlc_yellow_2024_01.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e1abb5",
   "metadata": {},
   "source": [
    "### Benchmark Test #1: efficiency gain by using cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440525ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "df_filtered = taxi_pq_df.filter(taxi_pq_df.passenger_count > 1) #Lazy execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede949f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check cache status\n",
    "print(df_filtered.storageLevel)\n",
    "print(df_filtered.is_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117a9976",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "result = df_filtered.count() #Action triggers real execution\n",
    "end = time.perf_counter()\n",
    "\n",
    "print(f\"Count WITHOUT cache: {(end - start)*1000:.2f} ms\")\n",
    "print(f\"{taxi_pq_df.count()} total rides - {result} rides with multiple passengers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e942182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Cache the dataframe and check execution time\n",
    "df_filtered.persist(StorageLevel.MEMORY_ONLY)\n",
    "print(df_filtered.storageLevel)\n",
    "print(df_filtered.is_cached)\n",
    "\n",
    "# first count triggers caching (caching is also LAZY just like filtering)\n",
    "# total time therefore is count-time + caching-time\n",
    "print(\"Count + cache:\")\n",
    "result = timer(lambda df: df.count(), df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44929e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second count should show speed gains by using cache:\n",
    "print(\"Count FROM cache:\")\n",
    "result = timer(lambda df: df.count(), df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1781255e",
   "metadata": {},
   "source": [
    "# Key takeaways: \n",
    "\n",
    "* Different cache modes between disk and in-memory, 1 or 2 replications\n",
    "* Caching/persisting is a LAZY operation, data only gets cached when an ACTION triggers it\n",
    "* Speed gains are minimal for small datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64620ef0",
   "metadata": {},
   "source": [
    "### Benchmark #2: Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84c9c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_partitions = spark.read.parquet(\"./nyc-yellow-taxi-trip-records-january-2024/nyc_tlc_yellow_2024_01.parquet\")\n",
    "df_partitions = df_partitions.filter(df_partitions.passenger_count > 1)\n",
    "\n",
    "print(f\"Partitions: {df_partitions.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Check partition sizes\n",
    "df_partitions.rdd.glom().map(len).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab63f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Count with 6 partitions:\")\n",
    "result = timer(lambda df: df.count(), df_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7f6cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repartitioned = df_partitions.repartition(12)\n",
    "\n",
    "# Check partition sizes\n",
    "df_repartitioned.rdd.glom().map(len).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cd653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Count with 12 partitions:\")\n",
    "result = timer(lambda df: df.count(), df_repartitioned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bb84f3",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "* not all partitions necessarely get used: after repartition(12) all partitions got 50k rows, before that some were empty\n",
    "* Spark GUI shows execution timing. 12 partitions = 12 tasks, my PC has 6 cores and i can see only 6 tasks ran concurrently, as expected\n",
    "* This means you can process as many partitions in parallel as you have available CPU cores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66a3186",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "\n",
    "### Check efficiency gains with larger datasets\n",
    "\n",
    "### Benchmark #3: Aggregations (groupBy, agg)\n",
    "\n",
    "check shuffle stage\n",
    "\n",
    "### Execution plans\n",
    "\n",
    "df.explain(\"extended\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
